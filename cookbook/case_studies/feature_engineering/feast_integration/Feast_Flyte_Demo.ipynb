{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8711cbe4",
   "metadata": {},
   "source": [
    "# Demo showcasing Flyte & Feast Integrationâ€”Feature Engineering and Training Pipeline\n",
    "\n",
    "In this demo, we will learn how to interact with Feast through Flyte. The goal will be to train a simple [Gaussian Naive Bayes model using sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html) on the [Horse-Colic dataset from UCI](https://archive.ics.uci.edu/ml/datasets/Horse+Colic).\n",
    "\n",
    "The model aims to classify if the lesion of the horse is surgical or not. It uses a modified version of the original dataset.\n",
    "\n",
    "**NOTE**\n",
    "We will not dive into the dataset or the model as the aim of this tutorial is to show how you can use Feast as a feature store and Flyte to engineer the features that can be identical across your online and offline training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79f28b4",
   "metadata": {},
   "source": [
    "### Step 1: Code ðŸ’»\n",
    "\n",
    "We have used [Flytekit](https://docs.flyte.org/projects/flytekit/en/latest/)â€”Flyte's Python SDK to express the pipeline in pure Python. The actual workflow code is auto-documented and rendered using sphinx [here](https://docs.flyte.org/projects/cookbook/en/latest/auto/case_studies/feature_engineering/feast_integration/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcea449",
   "metadata": {},
   "source": [
    "### Step 2: Launch an execution ðŸš€\n",
    "\n",
    "We can use [FlyteConsole](https://github.com/flyteorg/flyteconsole) to launch, monitor, and introspect Flyte executions. However, in our case, we will use [flytekit.remote](https://docs.flyte.org/projects/flytekit/en/latest/design/control_plane.html) to interact with the Flyte backend.\n",
    "\n",
    "#### Set up Flytekit remote from config\n",
    "\n",
    "To work with Flyte-sandbox, we need to create a simple local config at `~/.flyte/config`\n",
    "that points to Flyte-sandbox server and execution environment. We will initialize Flytekit remote with this server.\n",
    "\n",
    "Example configuration:\n",
    "```\n",
    "[platform]\n",
    "url = localhost:30081\n",
    "insecure = True\n",
    "```\n",
    "\n",
    "We will also pin FlyteRemote to one project and domain.\n",
    "\n",
    "**NOTE** The integration also sets up access to S3 or other equivalent datastores needed by FEAST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2330891",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from flytekit.remote import FlyteRemote\n",
    "remote = FlyteRemote.from_config(\"flytesnacks\", \"development\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d6295a",
   "metadata": {},
   "source": [
    "#### Retrieve the latest registered version of the pipeline\n",
    "\n",
    "FlyteRemote provides convenient methods to retrieve version of the pipeline from the remote server.\n",
    "\n",
    "**NOTE** It is possible to get a specific version of the workflow and trigger a launch for that, but let's just get the latest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28014f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lp = remote.fetch_launch_plan(name=\"feast_integration.feast_workflow.feast_workflow\")\n",
    "lp.id.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71210a7",
   "metadata": {},
   "source": [
    "#### Launch\n",
    "\n",
    "`remote.execute` simplifies starting an execution for the launch plan. Let's use the default inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13770fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "execution = remote.execute(lp, inputs={})\n",
    "print(f\"http://localhost:30081/console/projects/{execution.id.project}/domains/{execution.id.domain}/executions/{execution.id.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bd9e37",
   "metadata": {},
   "source": [
    "### Step 3: Wait for the execution to complete \n",
    "\n",
    "It is possible to launch a sync execution and wait for it to complete, but since all the processes are completely detached (you can even close your laptop and come back to it later), we will show how to sync the execution back.\n",
    "\n",
    "**Side Note**\n",
    "It is possible to fetch an existing execution or simply retrieve a started execution. Also, if you launch an execution with the same name, Flyte will respect that and not restart a new execution!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bd9614",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flytekit.models.core.execution import WorkflowExecutionPhase\n",
    "\n",
    "synced_execution = remote.sync(execution)\n",
    "print(f\"Execution {synced_execution.id.name} is in Phase - {WorkflowExecutionPhase.enum_to_string(synced_execution.closure.phase)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e5b181",
   "metadata": {},
   "source": [
    "### Step 4: Retrieve output\n",
    "\n",
    "Let's fetch the workflow outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab24b1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from feast_dataobjects import FeatureStore\n",
    "\n",
    "# \"raw_outputs\" in FlyteRemote helps associate type to the output, and resolves Literals to Python objects.\n",
    "# For example, a data class is returned as a marshmallow schema (serialized) when \"outputs\" is used but is returned as a data class when \"raw_outputs\" is used.\n",
    "fs = synced_execution.raw_outputs.get(\"o0\", FeatureStore)\n",
    "model = synced_execution.outputs['o1']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd55057",
   "metadata": {},
   "source": [
    "Next, we inspect the feature store configuration and model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31c96a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8277d3",
   "metadata": {},
   "source": [
    "**NOTE** The output model is available locally as a JobLibSerialized file, which can be downloaded and loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a841e22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a535b4d",
   "metadata": {},
   "source": [
    "### Step 5: Cool, let's predict\n",
    "\n",
    "We now have the model and feature store with us! So, how can we generate predictions? We can simply re-use the `predict` function from the workflow; Flytekit will automatically manage the IO for us.\n",
    "\n",
    "**NOTE** We set a couple of environment variables to facilitate the AWS access."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff58f63",
   "metadata": {},
   "source": [
    "#### Load features from an online feature store\n",
    "\n",
    "Let's re-use the feature definition from the Flyte workflow.\n",
    "\n",
    "```python\n",
    "inference_point = fs.get_online_features(FEAST_FEATURES, [{\"Hospital Number\": \"533738\"}])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a2c3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from feast_workflow import predict, FEAST_FEATURES\n",
    "\n",
    "os.environ[\"FLYTE_AWS_ENDPOINT\"] = os.environ[\"FEAST_S3_ENDPOINT_URL\"] = \"http://localhost:30084/\"\n",
    "os.environ[\"FLYTE_AWS_ACCESS_KEY_ID\"] = os.environ[\"AWS_ACCESS_KEY_ID\"] = \"minio\"\n",
    "os.environ[\"FLYTE_AWS_SECRET_ACCESS_KEY\"] = os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"miniostorage\"\n",
    "\n",
    "inference_point = fs.get_online_features(FEAST_FEATURES, [{\"Hospital Number\": \"533738\"}])\n",
    "\n",
    "inference_point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a49e572",
   "metadata": {},
   "source": [
    "#### Generate a prediction\n",
    "\n",
    "Notice how we are passing the serialized model and some loaded features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44c62e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(model_ser=model, features=inference_point)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b632e957",
   "metadata": {},
   "source": [
    "### Next Steps \n",
    "\n",
    "We can, of course, observe the intermediates from the workflow in the UI and download the intermediate data.\n",
    "\n",
    "### Future ðŸ”®\n",
    "\n",
    "We want to improve the integration experience further to allow the `predict` function to run in an inference server and a workflow. We are almost there, but we need to remove `model de-serialization` in the `predict` method."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dc3fb656270592a65285897570586647c2377dd9205211f8c7206e5246caf1a6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('feast': pyenv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
